<html>
<head>
<title>VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects</title>
<link rel="SHORTCUT ICON" href="favicon.ico"/>
<link href='css/paperstyle.css' rel='stylesheet' type='text/css'>
</head>

<body>

<div class="pageTitle">
    VAT-Mart: Learning Visual Action Trajectory <br>Proposals for Manipulating 3D ARTiculated Objects
  <br>
  <br>
  <span class = "Authors">
      <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://sxy7147.github.io/" target="_blank">Yan Zhao<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://www.cs.stanford.edu/~kaichun/" target="_blank">Kaichun Mo</a><sup>2*</sup> &nbsp; &nbsp;
      <a href="https://guozz.cn/" target="_blank">Zizheng Guo</a><sup>1</sup> &nbsp; &nbsp;
      <a href="https://galaxy-qazzz.github.io/" target="_blank">Yian Wang</a><sup>1</sup> &nbsp; &nbsp;<br>
      <a href="https://moistchi.github.io/" target="_blank">Tianhao Wu</a><sup>2</sup> &nbsp; &nbsp;
      <a href="https://fqnchina.github.io/" target="_blank">Qingnan Fan</a><sup>3</sup> &nbsp; &nbsp;
      <a href="https://xuelin-chen.github.io/" target="_blank">Xuelin Chen</a><sup>1</sup> &nbsp; &nbsp;
      <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">Leonidas J. Guibas</a><sup>2</sup> &nbsp; &nbsp;
      <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup> &nbsp; &nbsp;<br>
      <i>(*: indicates joint first authors)</i><br><br>
      <sup>1</sup><a href = "http://english.pku.edu.cn/" target="_blank"> Peking University </a> &nbsp; &nbsp;
      <sup>2</sup><a href = "http://www.stanford.edu/" target="_blank"> Stanford University </a> &nbsp; &nbsp;
      <sup>3</sup><a href = "https://ai.tencent.com/ailab/zh/index" target="_blank"> Tencent AI Lab </a> &nbsp; &nbsp;<br><br>
  </span>
  </div>
<br>
<div class = "material">
        <a href="https://arxiv.org/abs/2106.14440" target="_blank">[ArXiv Preprint]</a>
        <a href="paper.bib" target="_blank">[BibTex]</a> 
</div>

<div class = "abstractTitle">
  Abstract
  </div>
  <p class = "abstractText">
  Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data.
</p>

<div class="abstractTitle">
    Video Presentation [5-min]
</div>

<center>
      <iframe width="660" height="415" src="https://www.bilibili.com/video/BV13M4y1g7ms" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
</center>

  <img class = "bannerImage" src="images/c2f.png", width="800"><br>
  <table width="800" align="center"><tr><td><p class = "figureTitleText">
              Figure 1. <b>Coarse-to-Fine Part Assembly.</b> 
              Our iterative part assembly network learns to assemble parts to a shape in a coarse-to-fine manner.
  </p></td></tr></table>



<div class="abstractTitle">
    Network Architecture
</div>
  <img class = "bannerImage" src="images/network.png", width="800"><br>
  <table width="800" align="center"><tr><td><p class = "figureTitleText">
              Figure 2. <b>Network Architecture.</b> 
              The proposed dynamic graph learning framework. The iterative graph neural network backbone takes a set of part point clouds as inputs and conducts 5 iterations of graph message-passing for coarse-to-fine part assembly refinements.
  </p></td></tr></table>


<div class="abstractTitle">
    Qualitative Results
</div>
  <img class = "bannerImage" src="images/result.png", width="800"><br>
  <table width="800" align="center"><tr><td><p class = "figureTitleText">
              Figure 3. <b>Qualitative Comparisons to Baseline Methods.</b> 
              Visual comparisons between our algorithm and the baseline methods.
  </p></td></tr></table>


<div class="abstractTitle">
    Quantitative Comparisons
</div>
  <img class = "bannerImage" src="images/table.png", width="800"><br>
  <table width="800" align="center"><tr><td><p class = "figureTitleText">
              Figure 4. <b>Quantitative Comparisons to Baseline Methods.</b> 
              We show quantitative comparisons between our algorithm and the baseline methods.
  </p></td></tr></table>

  <div class="abstractTitle">
  Acknowledgements
  </div>
  <p class="abstractText">
This work was supported by the funding of National Natural Science Foundation of China â€”Youth Science Fund (No.62006006), a grant from the Toyota Research Institute University 2.0 program, NSF grant IIS-1763268, and a Vannevar Bush faculty fellowship.
We would like to thank Yourong Zhang for setting up ROS environment and helping in real robot experiments.
</p>

<p></p>

</body></html>
