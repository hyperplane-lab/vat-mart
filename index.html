<html>
<head>
<title>VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects</title>
<link rel="SHORTCUT ICON" href="favicon.ico"/>
<link href='css/paperstyle.css' rel='stylesheet' type='text/css'>
</head>

<body>

<div class="pageTitle">
    VAT-Mart: Learning Visual Action Trajectory <br>Proposals for Manipulating 3D ARTiculated Objects
  <br>
  <br>
  <span class = "Authors">
      <a href="https://warshallrho.github.io/" target="_blank">Ruihai Wu<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://sxy7147.github.io/" target="_blank">Yan Zhao<sup>1*</sup> &nbsp; &nbsp;
      <a href="https://www.cs.stanford.edu/~kaichun/" target="_blank">Kaichun Mo</a><sup>2*</sup> &nbsp; &nbsp;
      <a href="https://guozz.cn/" target="_blank">Zizheng Guo</a><sup>1</sup> &nbsp; &nbsp;
      <a href="https://galaxy-qazzz.github.io/" target="_blank">Yian Wang</a><sup>1</sup> &nbsp; &nbsp;<br>
      <a href="https://tianhaowuhz.github.io/" target="_blank">Tianhao Wu</a><sup>2</sup> &nbsp; &nbsp;
      <a href="https://fqnchina.github.io/" target="_blank">Qingnan Fan</a><sup>3</sup> &nbsp; &nbsp;
      <a href="https://xuelin-chen.github.io/" target="_blank">Xuelin Chen</a><sup>3</sup> &nbsp; &nbsp;
      <a href="https://geometry.stanford.edu/member/guibas/" target="_blank">Leonidas J. Guibas</a><sup>2</sup> &nbsp; &nbsp;
      <a href="https://zsdonghao.github.io/" target="_blank">Hao Dong</a><sup>1</sup> &nbsp; &nbsp;<br>
      <i>(*: indicates joint first authors)</i><br><br>
      <sup>1</sup><a href = "http://english.pku.edu.cn/" target="_blank"> Peking University </a> &nbsp; &nbsp;
      <sup>2</sup><a href = "http://www.stanford.edu/" target="_blank"> Stanford University </a> &nbsp; &nbsp;
      <sup>3</sup><a href = "https://ai.tencent.com/ailab/zh/index" target="_blank"> Tencent AI Lab </a> &nbsp; &nbsp;<br><br>
  </span>
  </div>
<br>
<div class = "material">
        <a href="https://iclr.cc/Conferences/2022" target="_blank">International Conference on Learning Representations (ICLR) 2022</a>
</div>
<br>
<div class = "material">
        <a href="https://openreview.net/pdf?id=iEx3PiooLy" target="_blank">[Paper]</a>
        <a href="https://github.com/warshallrho/VAT-Mart" target="_blank">[Code]</a>
        <a href="paper.bib" target="_blank">[BibTex]</a>
</div>

<div class = "abstractTitle">
  Abstract
  </div>
  <p class = "abstractText">
  Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in human environments is an important yet challenging task for future home-assistant robots. The space of 3D articulated objects is exceptionally rich in their myriad semantic categories, diverse shape geometry, and complicated part functionality. Previous works mostly abstract kinematic structure with estimated joint parameters and part poses as the visual representations for manipulating 3D articulated objects. In this paper, we propose object-centric actionable visual priors as a novel perception-interaction handshaking point that the perception system outputs more actionable guidance than kinematic structure estimation, by predicting dense geometry-aware, interaction-aware, and task-aware visual action affordance and trajectory proposals. We design an interaction-for-perception framework VAT-Mart to learn such actionable visual representations by simultaneously training a curiosity-driven reinforcement learning policy exploring diverse interaction trajectories and a perception module summarizing and generalizing the explored knowledge for pointwise predictions among diverse shapes. Experiments prove the effectiveness of the proposed approach using the large-scale PartNet-Mobility dataset in SAPIEN environment and show promising generalization capabilities to novel test shapes, unseen object categories, and real-world data.
</p>

<div class="abstractTitle">
    Video Presentation
</div>

<center>
    <iframe width="660" height="415" src="https://www.youtube.com/embed/HjhsLKf1eQY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <br><br><br>
    <iframe width="660" height="415" src="//player.bilibili.com/player.html?aid=723560870&bvid=BV1gS4y1y7hD&cid=494463403&page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</center>

<br>
<br>
<br>

<div class="abstractTitle">
    Tasks
</div>
  <img class="bannerImage" src="./images/teaser.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 1.
      Given an input 3D articulated object (a), we propose a novel perception-interaction handshaking point for robotic manipulation tasks - object-centric actionable visual priors, including per-point visual action affordance predictions (b) indicating where to interact, and diverse trajectory proposals (c) for selected contact points (marked with green dots) suggesting how to interact.
  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Framework and Network Architecture
</div>
  <img class="bannerImage" src="./images/fig2.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 2.
      Our proposed VAT-MART framework is composed of an RL policy (left) exploring interaction trajectories and a perception system (right) learning the desired actionable visual priors. We build bidirectional supervisory channels between the two parts: 1) the RL policy collects data to supervise the perception system, and 2) the perception system produces curiosity feedbacks encouraging the RL networks to explore diverse solutions.
  </p></td></tr></tbody></table>

<div class="abstractTitle">
    Qualitative Results
</div>
  <img class="bannerImage" src="./images/fig3.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 3.
      We show qualitative results of the actionability prediction and trajectory proposal modules. In each result block, from left to right, we present the input shape with the task, the predicted actionability heatmap, and three example trajectory proposals at a selected contact point.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/fig4.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 4.
      We present qualitative analysis of the learned trajectory scoring module. In each result block, from left to right, we show the input shape with the task, the input trajectory with its close-up view, and our network predictions of success likelihood applying the trajectory over all the points.
  </p></td></tr></tbody></table>
  <img class="bannerImage" src="./images/fig5.png" ,="" width="800"><br>
  <table width="800" align="center"><tbody><tr><td><p class="figureTitleText">
              Figure 5.
      Left: qualitative analysis of the trajectory scoring prediction (each column shares the same task; every row uses the same trajectory); Middle: promising results testing on real-world data (from left to right: input, affordance prediction, trajectory proposals); Right: real-robot experiment.
  </p></td></tr></tbody></table>


  <div class="abstractTitle">
  Acknowledgements
  </div>
  <p class="abstractText">
      National Natural Science Foundation of China â€”Youth Science Fund (No.62006006): Learning Visual Prediction of Interactive Physical Scenes using Unlabelled Videos.
Leonidas and Kaichun are supported by a grant from the Toyota Research Institute University 2.0 program, NSF grant IIS-1763268, and a Vannevar Bush faculty fellowship.

      We would like to thank <a href="https://blog.csdn.net/tritone" target="_blank">Yourong Zhang</a> for setting up ROS environment and helping in real robot experiments, <a href="https://github.com/KurtHorizon" target="_blank">Jiaqi Ke</a> for designing and doing heuristic experiments.
</p>

<p></p>

</body></html>
